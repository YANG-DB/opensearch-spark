version: "3"

services:
  spark-tutorial:
    image: tabulario/spark-iceberg
    container_name: spark-tutorial
    networks:
      iceberg_net:
    depends_on:
      - rest
      - minio
    volumes:
      - ./warehouse:/home/iceberg/warehouse
      - ./notebooks:/home/iceberg/notebooks/PPL
      - type: bind
        source: ./spark-defaults.conf
        target: /opt/spark/conf/spark-defaults.conf
      - type: bind
        source: $PPL_JAR
        target: /opt/spark/jars/ppl-spark-integration.jar
      - type: bind
        source: $FLINT_JAR
        target: /opt/spark/jars/flint-spark-integration.jar
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
    ports:
      - 8888:8888
      - 8080:8080
      - 10000:10000
      - 10001:10001
  rest:
    image: apache/iceberg-rest-fixture
    container_name: iceberg-rest
    networks:
      iceberg_net:
    ports:
      - 8181:8181
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
      - CATALOG_WAREHOUSE=s3://warehouse/
      - CATALOG_IO__IMPL=org.apache.iceberg.aws.s3.S3FileIO
      - CATALOG_S3_ENDPOINT=http://minio:9000
  minio:
    image: minio/minio
    container_name: minio
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=password
      - MINIO_DOMAIN=minio
    volumes:
      - minio-data:/data
    networks:
      iceberg_net:
        aliases:
          - warehouse.minio
    ports:
      - 9001:9001
      - 9000:9000
    command: ["server", "/data", "--console-address", ":9001"]
  mc:
    depends_on:
      - minio
    image: minio/mc
    container_name: mc
    networks:
      iceberg_net:
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
    entrypoint: >
      /bin/sh -c "
      until (/usr/bin/mc config host add minio http://minio:9000 admin password) do echo '...waiting...' && sleep 1; done;
      /usr/bin/mc rm -r --force minio/warehouse;
      /usr/bin/mc mb minio/warehouse;
      /usr/bin/mc policy set public minio/warehouse;
      tail -f /dev/null
      "
  opensearch:
    image: opensearchproject/opensearch:${OPENSEARCH_VERSION:-latest}
    container_name: opensearch
    environment:
      - cluster.name=opensearch-cluster
      - node.name=opensearch
      - discovery.seed_hosts=opensearch
      - cluster.initial_cluster_manager_nodes=opensearch
      - bootstrap.memory_lock=true
      - plugins.security.ssl.http.enabled=false
      - OPENSEARCH_JAVA_OPTS=-Xms${OPENSEARCH_NODE_MEMORY:-512m} -Xmx${OPENSEARCH_NODE_MEMORY:-512m}
      - OPENSEARCH_INITIAL_ADMIN_PASSWORD=${OPENSEARCH_ADMIN_PASSWORD}
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - opensearch-data:/usr/share/opensearch/data
      - ./opensearch/opensearch.yml:/usr/share/opensearch/config/opensearch.yml
      - ./opensearch/security/config:/usr/share/opensearch/plugins/opensearch-security/securityconfig
    ports:
      - ${OPENSEARCH_PORT:-9200}:9200
      - 9600:9600
    expose:
      - "${OPENSEARCH_PORT:-9200}"
    healthcheck:
      test: ["CMD", "curl", "-f", "-u", "admin:${OPENSEARCH_ADMIN_PASSWORD}", "http://localhost:9200/_cluster/health"]
      interval: 1m
      timeout: 5s
      retries: 3
    networks:
      iceberg_net:
  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:${DASHBOARDS_VERSION}
    container_name: opensearch-dashboards
    ports:
      - ${OPENSEARCH_DASHBOARDS_PORT:-5601}:5601
    expose:
      - "${OPENSEARCH_DASHBOARDS_PORT:-5601}"
    environment:
      OPENSEARCH_HOSTS: '["http://opensearch:9200"]'
    depends_on:
      - opensearch
    networks:
      iceberg_net:
networks:
  iceberg_net:

volumes:
  opensearch-data:
  minio-data: